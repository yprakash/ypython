{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09050192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d2a529",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d75a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('data/dogs-vs-cats')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a39cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the script below, we'll organize our data into train, validation, and test sets.\n",
    "# We'll do this by moving subsets of the data into sub-directories for each separate data set.\n",
    "dirs = ['train/dog', 'train/cat', 'valid/dog', 'valid/cat', 'test/dog', 'test/cat']\n",
    "\n",
    "if not os.path.isdir(dirs[0]):\n",
    "    for d in dirs:\n",
    "        os.makedirs(d)\n",
    "\n",
    "    for c in random.sample(glob.glob('cat*'), 500):\n",
    "        shutil.move(c, 'train/cat')\n",
    "    for c in random.sample(glob.glob('dog*'), 500):\n",
    "        shutil.move(c, 'train/dog')\n",
    "    for c in random.sample(glob.glob('cat*'), 100):\n",
    "        shutil.move(c, 'valid/cat')\n",
    "    for c in random.sample(glob.glob('dog*'), 100):\n",
    "        shutil.move(c, 'valid/dog')\n",
    "    for c in random.sample(glob.glob('cat*'), 50):\n",
    "        shutil.move(c, 'test/cat')\n",
    "    for c in random.sample(glob.glob('dog*'), 50):\n",
    "        shutil.move(c, 'test/dog')\n",
    "\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c0733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43234b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/dogs-vs-cats/train'\n",
    "test_path = 'data/dogs-vs-cats/test'\n",
    "valid_path = 'data/dogs-vs-cats/valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a76693",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n",
    "    .flow_from_directory(directory=train_path, target_size=(224,224), classes=['cat','dog'], batch_size=10)\n",
    "valid_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n",
    "    .flow_from_directory(directory=valid_path, target_size=(224,224), classes=['cat','dog'], batch_size=10)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n",
    "    .flow_from_directory(directory=test_path, target_size=(224,224), classes=['cat','dog'], batch_size=10, shuffle=False)\n",
    "\n",
    "# shuffle=False only for test_batches because, later when we plot the evaluation results from the model to a confusion\n",
    "# matrix, we'll need to able to access the unshuffled labels for the test set. By default, the data sets are shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8007abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_batches.n == 1000\n",
    "assert valid_batches.n == 200\n",
    "assert test_batches.n == 100\n",
    "assert train_batches.num_classes == valid_batches.num_classes == test_batches.num_classes == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3ff198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates a batch of images and labels from the training set.\n",
    "# the size of this batch is determined by the batch_size we set when we created train_batches.\n",
    "imgs, labels = next(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738279d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the processed images within our Jupyter notebook.\n",
    "# https://deeplizard.com/learn/video/LhEMXbjGV_4\n",
    "\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe69b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotImages(imgs)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f427f85",
   "metadata": {},
   "source": [
    "## Build and train a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c46e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D( # creates a convolution kernel that is wind with layers input which helps produce a tensor of outputs\n",
    "        # no. of filters that convolutional layers will learn from.\n",
    "        filters=32, # determines the number of output filters in the convolution\n",
    "        kernel_size=(3,3), # determines the dimensions of the kernel. (1, 1), (3, 3), (5, 5), or (7, 7) are mostly used.\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        input_shape=(224,224,3) # shape of input data, equals to target size\n",
    "    ),\n",
    "    MaxPool2D(pool_size=(2,2), strides=2), # strides: an integer or tuple/list of 2 integers,\n",
    "    # specifying the \"step\" of the convolution along with the height and width of the input volume.\n",
    "    Conv2D(\n",
    "        filters=64, # Layers early in the network architecture (i.e., closer to the actual input image) learn fewer convolutional\n",
    "            # filters while layers deeper in the network (i.e., closer to the output predictions) will learn more filters.\n",
    "        kernel_size=(3,3),\n",
    "        activation='relu',\n",
    "        padding='same'\n",
    "    ),\n",
    "    MaxPool2D(pool_size=(2,2), strides=2),\n",
    "    Flatten(),\n",
    "    Dense(units=2, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0f5c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=train_batches, validation_data=valid_batches, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be466a",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b92572",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs, test_labels = next(test_batches)\n",
    "plotImages(test_imgs)\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75320feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9524ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x=test_batches, verbose=0)\n",
    "np.round(predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://deeplizard.com/learn/video/bfQBPNDy5EM\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e07efde",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=test_batches.classes, y_pred=np.argmax(predictions, axis=-1))\n",
    "cm_plot_labels = ['cat','dog']\n",
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f85349",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14321a6",
   "metadata": {},
   "source": [
    "## Build fine-tuned VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34e0ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model with Internet connection\n",
    "vgg16_model = tf.keras.applications.vgg16.VGG16()\n",
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6449c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(vgg16_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f506e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# Add all layers of vgg16 except last one (which has 1000 classes)\n",
    "for layer in vgg16_model.layers[:-1]:\n",
    "    model.add(layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a17d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This freezes the weights and other trainable parameters, weights and biases in each layer so that\n",
    "# they will not be trained or updated when we later pass in our images of cats and dogs.\n",
    "# because vgg16 has already learned features of cats and dogs in its original training\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86479ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3031834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable, model.layers[-2].trainable, model.layers[-1].trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7a2556",
   "metadata": {},
   "source": [
    "### Train the fine-tuned vgg16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679c8b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2098edfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=train_batches, validation_data=valid_batches, epochs=5, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26beb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model.history.history.get('accuracy')[-1] > 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c98987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('custom_vgg16_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
